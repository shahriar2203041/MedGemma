{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYba2hfAs0AS"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kILpWz4Gs5Kh"
      },
      "source": [
        "# Quick start with Model Garden - MedGemma\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogle-Health%2Fmedgemma%2Fmain%2Fnotebooks%2Fquick_start_with_model_garden.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/google-health/medgemma/blob/main/notebooks/quick_start_with_model_garden.ipynb\">\n",
        "      <img alt=\"Google Colab logo\" src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" width=\"32px\"><br> Run in Google Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/google-health/medgemma/blob/main/notebooks/quick_start_with_model_garden.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRGljGF2lUWd"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to use MedGemma in Vertex AI to generate responses from medical text and images using two methods for getting inferences:\n",
        "\n",
        "* **Online inferences** are synchronous requests that are made to the endpoint deployed from Model Garden and are served with low latency. Online inferences are useful if the model outputs are being used in production. The cost for online inference is based on the time a virtual machine spends waiting in an active state (an endpoint with a deployed model) to handle inference requests.\n",
        "\n",
        "* **Batch inferences** are asynchronous requests that are run on a set number of model inputs specified in a single job. They are made directly to an uploaded model and do not use an endpoint deployed from Model Garden. Batch inferences are useful if you want to get accumulated inference results for a large number of inputs for use in training and do not require low latency. The cost for batch inference is based on the time a virtual machine spends running your inference job.\n",
        "\n",
        "Vertex AI makes it easy to serve your model and make it accessible to the world. Learn more about [Vertex AI](https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform).\n",
        "\n",
        "### Objectives\n",
        "\n",
        "- Deploy MedGemma to a Vertex AI Endpoint and get online inferences.\n",
        "- Upload MedGemma to Vertex AI Model Registry and get batch inferences.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emPr6M1fs5Kj"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ag_zmUlgmJD8"
      },
      "outputs": [],
      "source": [
        "# @title Import packages and define common functions\n",
        "\n",
        "import datetime\n",
        "import importlib\n",
        "import json\n",
        "import os\n",
        "import requests\n",
        "import sys\n",
        "import uuid\n",
        "\n",
        "from google.cloud import aiplatform, storage\n",
        "from IPython.display import Image as IPImage, display, Markdown\n",
        "import google.auth\n",
        "import openai\n",
        "\n",
        "if not os.path.isdir(\"vertex-ai-samples\"):\n",
        "    ! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "models, endpoints = {}, {}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Authenticate with Google Cloud (only needed for Google Colab)\n",
        "\n",
        "google_colab = \"google.colab\" in sys.modules and not os.environ.get(\"VERTEX_PRODUCT\") == \"COLAB_ENTERPRISE\"\n",
        "\n",
        "if google_colab:\n",
        "    from google.colab import auth\n",
        "    # There will be a popup asking you to sign in with your user account and\n",
        "    # approve access\n",
        "    auth.authenticate_user()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "61esx7taKAWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zMsN9Ep1mJD8"
      },
      "outputs": [],
      "source": [
        "# @title Set up Google Cloud environment\n",
        "\n",
        "# @markdown #### Prerequisites\n",
        "# @markdown 1. To get started with Vertex AI, you will need an existing Google Cloud project with the Vertex AI API enabled. Refer to the guide to [set up a project and development environment](https://docs.cloud.google.com/vertex-ai/docs/start/cloud-environment).\n",
        "# @markdown 2. Confirm that [billing is enabled](https://cloud.google.com/billing/docs/how-to/modify-project) for your project.\n",
        "# @markdown 3. Make sure that you have the following required roles:\n",
        "# @markdown    - [Vertex AI User](https://cloud.google.com/vertex-ai/docs/general/access-control?_gl=1*j68dlr*_ga*MTExMjk3OTcwMy4xNzY4MzcyMzc2*_ga_WH2QY8WWF5*czE3Njg0MzAxNTYkbzUkZzEkdDE3Njg0MzU4NjUkajYwJGwwJGgw#aiplatform.user) (`roles/aiplatform.user`)\n",
        "# @markdown    - [Colab Enterprise User](https://cloud.google.com/vertex-ai/docs/general/access-control?_gl=1*j68dlr*_ga*MTExMjk3OTcwMy4xNzY4MzcyMzc2*_ga_WH2QY8WWF5*czE3Njg0MzAxNTYkbzUkZzEkdDE3Njg0MzU4NjUkajYwJGwwJGgw#aiplatform.colabEnterpriseUser) (`roles/aiplatform.colabEnterpriseUser`), if you intend to run this notebook in Colab Enterprise (not needed for Google Colab)\n",
        "# @markdown 4. Ensure that either the Compute Engine API is enabled or that you have the [Service Usage Admin](https://cloud.google.com/iam/docs/understanding-roles#serviceusage.serviceUsageAdmin) (`roles/serviceusage.serviceUsageAdmin`) role to enable the API.\n",
        "\n",
        "# @markdown This section sets the Google Cloud project and region, enables the Vertex AI and Compute Engine APIs (if not already enabled), and initializes the Vertex AI API.\n",
        "# @markdown\n",
        "# @markdown Set your Google Cloud project ID and region below. If you are running this notebook in Colab Enterprise, you can leave the fields blank to use the default project ID and region from the notebook environment.\n",
        "PROJECT_ID = \"\"  # @param {type: \"string\", placeholder: \"e.g. your-project\"}\n",
        "\n",
        "# Get the default project ID from the environment\n",
        "if not PROJECT_ID:\n",
        "    PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "os.environ[\"CLOUDSDK_CORE_PROJECT\"] = PROJECT_ID\n",
        "\n",
        "REGION = \"\"  # @param {type: \"string\", placeholder: \"e.g. us-central1\"}\n",
        "\n",
        "# Get the default region for launching jobs from the environment\n",
        "if not REGION:\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI and Compute Engine APIs, if not already enabled\n",
        "print(\"Enabling Vertex AI API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com\n",
        "print(\"Enabling Compute Engine API.\")\n",
        "! gcloud services enable compute.googleapis.com\n",
        "\n",
        "# Initialize Vertex AI API\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, api_transport=\"rest\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bak-klNTmJD8"
      },
      "source": [
        "## Get online inferences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjOWCeGJu-94",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Set up Vertex AI endpoint\n",
        "\n",
        "# @markdown To get [online inferences](https://cloud.google.com/vertex-ai/docs/predictions/get-online-predictions), you will need a MedGemma [Vertex AI endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment) that has been deployed from Model Garden. If you have not already done so, go to the [MedGemma model card](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/medgemma) and click **\"Deploy model\"** to deploy the model.\n",
        "# @markdown\n",
        "# @markdown **Note:** The examples in this notebook are intended to be used with instruction-tuned variants. Make sure to use an instruction-tuned model variant to run this notebook.\n",
        "# @markdown\n",
        "# @markdown This section gets the Vertex AI endpoint resource that you deployed from Model Garden to use for online inferences.\n",
        "# @markdown\n",
        "# @markdown Fill in the endpoint ID and region below. You can find your deployed endpoint on the [Vertex AI endpoints page](https://console.cloud.google.com/vertex-ai/online-prediction/endpoints).\n",
        "\n",
        "ENDPOINT_ID = \"\"  # @param {type: \"string\", placeholder: \"e.g. 123456789\"}\n",
        "ENDPOINT_REGION = \"\"  # @param {type: \"string\", placeholder: \"e.g. us-central1\"}\n",
        "\n",
        "endpoints[\"endpoint\"] = aiplatform.Endpoint(\n",
        "    endpoint_name=ENDPOINT_ID,\n",
        "    location=ENDPOINT_REGION,\n",
        ")\n",
        "\n",
        "# Use the endpoint name to check that you are using an appropriate model variant\n",
        "# and settings. These checks are based on the default endpoint name from the\n",
        "# Model Garden deployment settings.\n",
        "ENDPOINT_NAME = endpoints[\"endpoint\"].display_name\n",
        "if \"pt\" in ENDPOINT_NAME:\n",
        "    raise ValueError(\n",
        "        \"The examples in this notebook are intended to be used with \"\n",
        "        \"instruction-tuned variants. Please use an instruction-tuned model.\"\n",
        "    )\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown Set `use_dedicated_endpoint` if you are using a [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/predictions/choose-endpoint-type) (`True` by default for Model Garden deployments). Uncheck this option for all other endpoint types.\n",
        "\n",
        "use_dedicated_endpoint = True  # @param {type: \"boolean\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown MedGemma 1.5 4B and MedGemma 27B variants support thinking. If enabled, a system instruction will be added to prompt the model to output its reasoning between two special tokens `<unused94>` and `<unused95>` followed by the final response. **Note:** This notebook shows how to enable thinking for any multimodal or text-only task but it may not benefit some use cases. Thinking may potentially improve results on challenging text-only tasks at the cost of generating more tokens. Thoroughly validate the model's performance and assess the inference time and cost when enabling thinking.\n",
        "is_thinking = False  # @param {type: \"boolean\"}\n",
        "THINKING_VARIANTS = [\n",
        "    \"medgemma-1.5-4b-it\",\n",
        "    \"medgemma-27b-it\",\n",
        "    \"medgemma-27b-text-it\",\n",
        "]\n",
        "if is_thinking and not any(variant in ENDPOINT_NAME for variant in THINKING_VARIANTS):\n",
        "    is_thinking = False\n",
        "    print(\n",
        "        \"Note: Thinking is enabled for a non-thinking variant. Setting \"\n",
        "        \"`is_thinking` to `False`.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewKYdFHbu-94"
      },
      "source": [
        "### Run inference on images and text\n",
        "\n",
        "This section demonstrates running inference on image-based tasks using multimodal variants.\n",
        "\n",
        "**Note:** Proceed to [Run inference on text only](#scrollTo=lRQDWe2znWn7) if you have selected the 27B text-only variant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZSj2CNKhu-94"
      },
      "outputs": [],
      "source": [
        "# @title #### Specify image and text inputs\n",
        "\n",
        "# Check that you are using a multimodal variant\n",
        "if \"text\" in ENDPOINT_NAME:\n",
        "    raise ValueError(\n",
        "        \"You are using a text-only variant which does not support multimodal\"\n",
        "        \" inputs. Proceed to the 'Run inference on text only' section.\"\n",
        "    )\n",
        "\n",
        "system_instruction = \"You are an expert radiologist.\"\n",
        "prompt = \"Describe this X-ray\" # @param {type: \"string\"}\n",
        "image_url = \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\" # @param {type: \"string\"}\n",
        "! wget -nc -q {image_url}\n",
        "image_filename = os.path.basename(image_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bvvvJpZNolJk"
      },
      "outputs": [],
      "source": [
        "# @title #### Format conversation\n",
        "\n",
        "if is_thinking:\n",
        "    system_instruction = f\"SYSTEM INSTRUCTION: think silently if needed. {system_instruction}\"\n",
        "    max_tokens = 1500\n",
        "else:\n",
        "    max_tokens = 500\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": [{\"type\": \"text\", \"text\": system_instruction}]\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": prompt},\n",
        "            {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n",
        "        ]\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhijNE6PnWn7",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title #### Generate responses using the Vertex AI SDK\n",
        "\n",
        "# @markdown This section shows how to send [online inference](https://cloud.google.com/vertex-ai/docs/predictions/get-online-predictions) requests to the Vertex AI endpoint.\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details.\n",
        "\n",
        "display(Markdown(f\"---\\n\\n**[ User ]**\\n\\n{prompt}\"))\n",
        "display(IPImage(filename=image_filename, height=300))\n",
        "\n",
        "instances = [\n",
        "    {\n",
        "        \"@requestFormat\": \"chatCompletions\",\n",
        "        \"messages\": messages,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": 0\n",
        "    },\n",
        "]\n",
        "\n",
        "response = endpoints[\"endpoint\"].predict(\n",
        "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
        ").predictions[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "if is_thinking:\n",
        "    thought, response = response.split(\"<unused95>\")\n",
        "    thought = thought.replace(\"<unused94>thought\\n\", \"\")\n",
        "    display(Markdown(f\"---\\n\\n**[ MedGemma thinking trace ]**\\n\\n{thought}\"))\n",
        "display(Markdown(f\"---\\n\\n**[ MedGemma ]**\\n\\n{response}\\n\\n---\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "za9B4jD6nWn7"
      },
      "outputs": [],
      "source": [
        "# @title #### Generate responses using the OpenAI SDK\n",
        "\n",
        "# @markdown This section shows how to send [chat completions](https://platform.openai.com/docs/api-reference/chat) requests to the endpoint using the OpenAI SDK.\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details.\n",
        "\n",
        "if \"dicom\" in ENDPOINT_NAME:\n",
        "    raise NotImplementedError(\n",
        "        \"MedGemma DICOM endpoints are currently not OpenAI-compatible. Use the \"\n",
        "        \"Vertex AI SDK to run inference.\"\n",
        "    )\n",
        "\n",
        "display(Markdown(f\"---\\n\\n**[ User ]**\\n\\n{prompt}\"))\n",
        "display(IPImage(filename=image_filename, height=300))\n",
        "\n",
        "creds, project = google.auth.default()\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "creds.refresh(auth_req)\n",
        "\n",
        "ENDPOINT_RESOURCE_NAME = endpoints[\"endpoint\"].resource_name\n",
        "\n",
        "if use_dedicated_endpoint:\n",
        "    DEDICATED_ENDPOINT_DNS = endpoints[\"endpoint\"].gca_resource.dedicated_endpoint_dns\n",
        "    BASE_URL = f\"https://{DEDICATED_ENDPOINT_DNS}/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        "else:\n",
        "    BASE_URL = f\"https://{ENDPOINT_REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        "\n",
        "client = openai.OpenAI(base_url=BASE_URL, api_key=creds.token)\n",
        "\n",
        "model_response = client.chat.completions.create(\n",
        "    model=\"\",\n",
        "    messages=messages,\n",
        "    max_completion_tokens=max_tokens,\n",
        "    temperature=0,\n",
        ")\n",
        "response = model_response.choices[0].message.content\n",
        "\n",
        "if is_thinking:\n",
        "    thought, response = response.split(\"<unused95>\")\n",
        "    thought = thought.replace(\"<unused94>thought\\n\", \"\")\n",
        "    display(Markdown(f\"---\\n\\n**[ MedGemma thinking trace ]**\\n\\n{thought}\"))\n",
        "display(Markdown(f\"---\\n\\n**[ MedGemma ]**\\n\\n{response}\\n\\n---\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRQDWe2znWn7"
      },
      "source": [
        "### Run inference on text only\n",
        "\n",
        "This section demonstrates running inference on text-based tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Hty-REwnnWn7"
      },
      "outputs": [],
      "source": [
        "# @title #### Specify text prompt\n",
        "\n",
        "system_instruction = \"You are a helpful medical assistant.\"\n",
        "prompt = \"How do you differentiate bacterial from viral pneumonia?\"  # @param {type:\"string\"}\n",
        "\n",
        "if is_thinking:\n",
        "    system_instruction = f\"SYSTEM INSTRUCTION: think silently if needed. {system_instruction}\"\n",
        "    max_tokens = 2000\n",
        "else:\n",
        "    max_tokens = 1000\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_instruction\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Mnr7tpAznWn7"
      },
      "outputs": [],
      "source": [
        "# @title #### Generate responses using the Vertex AI SDK\n",
        "\n",
        "# @markdown This section shows how to send [online inference](https://cloud.google.com/vertex-ai/docs/predictions/get-online-predictions) requests to the Vertex AI endpoint.\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details.\n",
        "\n",
        "display(Markdown(f\"---\\n\\n**[ User ]**\\n\\n{prompt}\\n\\n---\"))\n",
        "\n",
        "instances = [\n",
        "    {\n",
        "        \"@requestFormat\": \"chatCompletions\",\n",
        "        \"messages\": messages,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": 0\n",
        "    },\n",
        "]\n",
        "\n",
        "response = endpoints[\"endpoint\"].predict(\n",
        "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
        ").predictions[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "if is_thinking:\n",
        "    thought, response = response.split(\"<unused95>\")\n",
        "    thought = thought.replace(\"<unused94>thought\\n\", \"\")\n",
        "    display(Markdown(f\"**[ MedGemma thinking trace ]**\\n\\n{thought}\\n\\n---\"))\n",
        "display(Markdown(f\"**[ MedGemma ]**\\n\\n{response}\\n\\n---\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "h_p81loVnWn7"
      },
      "outputs": [],
      "source": [
        "# @title #### Generate responses using the OpenAI SDK\n",
        "\n",
        "# @markdown This section shows how to send [chat completions](https://platform.openai.com/docs/api-reference/chat) requests to the endpoint using the OpenAI SDK.\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details.\n",
        "\n",
        "if \"dicom\" in ENDPOINT_NAME:\n",
        "    raise NotImplementedError(\n",
        "        \"MedGemma DICOM endpoints are currently not OpenAI-compatible. Use the \"\n",
        "        \"Vertex AI SDK to run inference.\"\n",
        "    )\n",
        "\n",
        "display(Markdown(f\"---\\n\\n**[ User ]**\\n\\n{prompt}\\n\\n---\"))\n",
        "\n",
        "creds, project = google.auth.default()\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "creds.refresh(auth_req)\n",
        "\n",
        "ENDPOINT_RESOURCE_NAME = endpoints[\"endpoint\"].resource_name\n",
        "\n",
        "if use_dedicated_endpoint:\n",
        "    DEDICATED_ENDPOINT_DNS = endpoints[\"endpoint\"].gca_resource.dedicated_endpoint_dns\n",
        "    BASE_URL = f\"https://{DEDICATED_ENDPOINT_DNS}/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        "else:\n",
        "    BASE_URL = f\"https://{ENDPOINT_REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        "\n",
        "client = openai.OpenAI(base_url=BASE_URL, api_key=creds.token)\n",
        "\n",
        "model_response = client.chat.completions.create(\n",
        "    model=\"\",\n",
        "    messages=messages,\n",
        "    max_completion_tokens=max_tokens,\n",
        "    temperature=0,\n",
        ")\n",
        "response = model_response.choices[0].message.content\n",
        "\n",
        "if is_thinking:\n",
        "    thought, response = response.split(\"<unused95>\")\n",
        "    thought = thought.replace(\"<unused94>thought\\n\", \"\")\n",
        "    display(Markdown(f\"**[ MedGemma thinking trace ]**\\n\\n{thought}\\n\\n---\"))\n",
        "display(Markdown(f\"**[ MedGemma ]**\\n\\n{response}\\n\\n---\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuUQXo4EBqt7"
      },
      "source": [
        "### Stream responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zaVVHkX_-PfL"
      },
      "outputs": [],
      "source": [
        "# @markdown This section demonstrates streaming responses, which lets you display or process the model output in chunks while the full response is being generated.\n",
        "# @markdown\n",
        "# @markdown You can stream responses directly using the Vertex AI endpoint (set `use_vertex` to `True`) or alternatively using the OpenAI SDK (set to `False`).\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details.\n",
        "\n",
        "if \"dicom\" in ENDPOINT_NAME:\n",
        "    raise NotImplementedError(\n",
        "        \"MedGemma DICOM endpoints currently do not support streaming.\"\n",
        "    )\n",
        "\n",
        "use_vertex = True  # @param {type: \"boolean\"}\n",
        "\n",
        "prompt = \"How do you differentiate bacterial from viral pneumonia?\"\n",
        "display(Markdown(f\"---\\n\\n**[ User ]**\\n\\n{prompt}\\n\\n---\\n\\n**[MedGemma]**\"))\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful medical assistant.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "max_tokens = 1500\n",
        "\n",
        "creds, project = google.auth.default()\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "creds.refresh(auth_req)\n",
        "\n",
        "ENDPOINT_RESOURCE_NAME = endpoints[\"endpoint\"].resource_name\n",
        "\n",
        "if use_dedicated_endpoint:\n",
        "    DEDICATED_ENDPOINT_DNS = endpoints[\"endpoint\"].gca_resource.dedicated_endpoint_dns\n",
        "    BASE_URL = f\"https://{DEDICATED_ENDPOINT_DNS}/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        "else:\n",
        "    BASE_URL = f\"https://{ENDPOINT_REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        "\n",
        "if use_vertex:\n",
        "    endpoint_url = f\"{BASE_URL}/chat/completions\"\n",
        "    request = {\n",
        "        \"messages\": messages,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": 0,\n",
        "        \"stream\": True\n",
        "    }\n",
        "    with requests.post(\n",
        "        endpoint_url,\n",
        "        headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
        "        json=request,\n",
        "        stream=True,\n",
        "    ) as r:\n",
        "        for chunk in r.iter_lines(chunk_size=8192):\n",
        "            if chunk:\n",
        "                chunk = chunk.decode(\"utf-8\").removeprefix(\"data:\").strip()\n",
        "                if chunk == \"[DONE]\":\n",
        "                    break\n",
        "                chunk = json.loads(chunk)\n",
        "                print(chunk[\"choices\"][0][\"delta\"][\"content\"], sep=\" \", end=\"\")\n",
        "else:\n",
        "    client = openai.OpenAI(base_url=BASE_URL, api_key=creds.token)\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"\",\n",
        "        messages=messages,\n",
        "        max_completion_tokens=max_tokens,\n",
        "        temperature=0,\n",
        "        stream=True,\n",
        "    )\n",
        "    for chunk in response:\n",
        "        print(chunk.choices[0].delta.content, sep=\" \", end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGxnyDNqnWn7"
      },
      "source": [
        "## Get batch inferences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "k-W1lcSxnWn7"
      },
      "outputs": [],
      "source": [
        "# @title Get access to MedGemma\n",
        "\n",
        "# @markdown The inference container directly loads the model from Hugging Face Hub.\n",
        "# @markdown\n",
        "# @markdown To enable access to the MedGemma models, you must provide a Hugging Face User Access Token. You can follow the [Hugging Face documentation](https://huggingface.co/docs/hub/en/security-tokens) to create a **read** access token and specify it in the `HF_TOKEN` field below.\n",
        "\n",
        "HF_TOKEN = \"\"  # @param {type: \"string\", placeholder: \"Hugging Face User Access Token\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tiAf4z0X5ErU"
      },
      "outputs": [],
      "source": [
        "# @title Upload model to Vertex AI Model Registry\n",
        "\n",
        "# @markdown To get [batch inferences](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions), you must first upload the prebuilt MedGemma model to [Vertex AI Model Registry](https://cloud.google.com/vertex-ai/docs/model-registry/introduction). Batch inference requests are made directly to a model in Model Registry without deploying to an endpoint.\n",
        "\n",
        "# Note: Batch inferences may not work for the 27B variants so they are not included in the dropdown.\n",
        "MODEL_VARIANT = \"4b-it\"  # @param [\"4b-it\"]\n",
        "\n",
        "MODEL_ID = f\"medgemma-{MODEL_VARIANT}\"\n",
        "\n",
        "# The pre-built serving docker image.\n",
        "SERVE_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20250430_0916_RC00_maas\"\n",
        "\n",
        "# This notebook uses Nvidia L4 GPUs for demonstration.\n",
        "# See https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#batch_prediction\n",
        "# for details on configuring compute for Vertex AI batch inferences.\n",
        "if \"4b\" in MODEL_ID:\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "    machine_type = \"g2-standard-24\"\n",
        "    accelerator_count = 2\n",
        "elif \"27b\" in MODEL_ID:\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "    machine_type = \"g2-standard-48\"\n",
        "    accelerator_count = 4\n",
        "else:\n",
        "    raise ValueError(\n",
        "        f\"Recommended machine settings not found for model: {MODEL_ID}.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def upload_model(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    accelerator_count: int = 1,\n",
        "    gpu_memory_utilization: float = 0.95,\n",
        "    max_model_len: int = 32768,\n",
        "    max_num_seqs: int = 16,\n",
        "    max_images: int = 16,\n",
        ") -> aiplatform.Model:\n",
        "\n",
        "    vllm_args = [\n",
        "        \"python\",\n",
        "        \"-m\",\n",
        "        \"vllm.entrypoints.api_server\",\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=8080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tensor-parallel-size={accelerator_count}\",\n",
        "        \"--swap-space=16\",\n",
        "        f\"--gpu-memory-utilization={gpu_memory_utilization}\",\n",
        "        f\"--max-model-len={max_model_len}\",\n",
        "        f\"--max-num-seqs={max_num_seqs}\",\n",
        "        \"--enable-chunked-prefill\",\n",
        "        \"--disable-log-stats\",\n",
        "    ]\n",
        "\n",
        "    if \"text\" not in model_id:\n",
        "        vllm_args.extend([\n",
        "            f\"--limit_mm_per_prompt='image={max_images}'\",\n",
        "            \"--mm-processor-kwargs='{\\\"do_pan_and_scan\\\": true}'\"\n",
        "        ])\n",
        "\n",
        "\n",
        "    env_vars = {\n",
        "        \"MODEL_ID\": model_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "        \"VLLM_USE_V1\": \"0\",\n",
        "        \"HF_TOKEN\": HF_TOKEN,\n",
        "    }\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=SERVE_DOCKER_URI,\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[8080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "models[\"model\"] = upload_model(\n",
        "    model_name=common_util.get_job_name_with_datetime(prefix=MODEL_ID),\n",
        "    model_id=f\"google/{MODEL_ID}\",\n",
        "    accelerator_count=accelerator_count,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6X7ua4XqnWn7",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Set up Google Cloud resources\n",
        "\n",
        "# @markdown This section sets up a [Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing batch inference inputs and outputs and gets the [Compute Engine default service account](https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) which will be used to run the batch inference jobs.\n",
        "\n",
        "# @markdown 1. Make sure that you have the following required roles:\n",
        "# @markdown - [Storage Admin](https://cloud.google.com/iam/docs/understanding-roles#storage.admin) (`roles/storage.admin`) to create and use Cloud Storage buckets\n",
        "# @markdown - [Service Account User](https://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountUser) (`roles/iam.serviceAccountUser`) on either the project or the Compute Engine default service account\n",
        "\n",
        "# @markdown 2. Set up a Cloud Storage bucket.\n",
        "# @markdown - A new bucket will automatically be created for you.\n",
        "# @markdown - [Optional] To use an existing bucket, specify the `gs://` bucket URI. The specified Cloud Storage bucket should be located in the same region as where the notebook was launched. Note that a multi-region bucket (e.g. \"us\") is not considered a match for a single region (e.g. \"us-central1\") covered by the multi-region range.\n",
        "\n",
        "BUCKET_URI = \"\"  # @param {type: \"string\", placeholder: \"[Optional] Cloud Storage bucket URI\"}\n",
        "\n",
        "# Cloud Storage bucket for storing batch inference artifacts.\n",
        "# A unique bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, change the value of BUCKET_URI above.\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\":\n",
        "    now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    ! gcloud storage buckets create --location {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    shell_output = ! gcloud storage buckets describe {BUCKET_NAME} | grep \"location:\" | sed \"s/location://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            f\"Bucket region {bucket_region} is different from notebook region {REGION}\"\n",
        "        )\n",
        "print(f\"Using this Cloud Storage Bucket: {BUCKET_URI}\")\n",
        "\n",
        "# Service account used for running the inference container.\n",
        "# Gets the Compute Engine default service account. If you prefer using your own\n",
        "# custom service account, change the value of SERVICE_ACCOUNT below.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this service account:\", SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0QuVXUrnWn7"
      },
      "source": [
        "### Predict\n",
        "\n",
        "You can send [batch inference requests](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions#request_a_batch_prediction) to the model using a [JSON Lines](https://jsonlines.org/) file to specify a list of input instances with text prompts and images to generate output. For more details on configuring batch inference jobs, see how to [format your input data](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions#input_data_requirements) and [choose compute settings](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions#choose_machine_type_and_replica_count)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YpbLjzbp7TC_"
      },
      "outputs": [],
      "source": [
        "# @title Generate responses in batch from images and text\n",
        "\n",
        "# @markdown This section demonstrates running inference in batch on image-based tasks using multimodal variants.\n",
        "# @markdown\n",
        "# @markdown **Note:** Proceed to [Generate responses in batch from text only](#scrollTo=sbMkoiJ161hO) if you have selected the 27B text-only variant.\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details.\n",
        "\n",
        "# Check that you are using a multimodal variant\n",
        "if \"text\" in MODEL_VARIANT:\n",
        "    raise ValueError(\n",
        "        \"You are using a text-only variant which does not support multimodal \"\n",
        "        \"inputs. Please proceed to the 'Generate responses in batch from text \"\n",
        "        \"only' section.\"\n",
        "    )\n",
        "\n",
        "batch_predict_instances = [\n",
        "    {\n",
        "        \"@requestFormat\": \"chatCompletions\",\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": [{\"type\": \"text\", \"text\": \"You are an expert radiologist.\"}]\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": \"Describe this X-ray\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\"url\": \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\"}\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        \"max_tokens\": 200\n",
        "    }\n",
        "]\n",
        "\n",
        "# Write instances to JSON Lines file\n",
        "os.makedirs(\"batch_predict_input\", exist_ok=True)\n",
        "instances_filename = \"multimodal_instances.jsonl\"\n",
        "with open(f\"batch_predict_input/{instances_filename}\", \"w\") as f:\n",
        "  for line in batch_predict_instances:\n",
        "    json_str = json.dumps(line)\n",
        "    f.write(json_str)\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "# Copy the file to Cloud Storage\n",
        "batch_predict_prefix = f\"batch-predict-{MODEL_ID}\"\n",
        "! gcloud storage cp ./batch_predict_input/{instances_filename} {BUCKET_URI}/{batch_predict_prefix}/input/{instances_filename}\n",
        "\n",
        "batch_predict_job_name = common_util.get_job_name_with_datetime(\n",
        "    prefix=f\"batch-predict-{MODEL_ID}\"\n",
        ")\n",
        "\n",
        "multimodal_batch_predict_job = models[\"model\"].batch_predict(\n",
        "    job_display_name=batch_predict_job_name,\n",
        "    gcs_source=os.path.join(\n",
        "        BUCKET_URI, batch_predict_prefix, f\"input/{instances_filename}\"\n",
        "    ),\n",
        "    gcs_destination_prefix=os.path.join(\n",
        "        BUCKET_URI, batch_predict_prefix, \"output\"\n",
        "    ),\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        ")\n",
        "\n",
        "multimodal_batch_predict_job.wait()\n",
        "\n",
        "print(multimodal_batch_predict_job.display_name)\n",
        "print(multimodal_batch_predict_job.resource_name)\n",
        "print(multimodal_batch_predict_job.state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sbMkoiJ161hO"
      },
      "outputs": [],
      "source": [
        "# @title Generate responses in batch from text only\n",
        "\n",
        "# @markdown This section demonstrates running inference in batch on text-based tasks.\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details.\n",
        "\n",
        "batch_predict_instances = [\n",
        "    {\n",
        "        \"@requestFormat\": \"chatCompletions\",\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful medical assistant.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"How do you differentiate bacterial from viral pneumonia?\"\n",
        "            }\n",
        "        ],\n",
        "        \"max_tokens\": 200\n",
        "    }\n",
        "]\n",
        "\n",
        "# Write instances to JSON Lines file\n",
        "os.makedirs(\"batch_predict_input\", exist_ok=True)\n",
        "instances_filename = \"text_instances.jsonl\"\n",
        "with open(f\"batch_predict_input/{instances_filename}\", \"w\") as f:\n",
        "  for line in batch_predict_instances:\n",
        "    json_str = json.dumps(line)\n",
        "    f.write(json_str)\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "# Copy the file to Cloud Storage\n",
        "batch_predict_prefix = f\"batch-predict-{MODEL_ID}\"\n",
        "! gcloud storage cp ./batch_predict_input/{instances_filename} {BUCKET_URI}/{batch_predict_prefix}/input/{instances_filename}\n",
        "\n",
        "batch_predict_job_name = common_util.get_job_name_with_datetime(\n",
        "    prefix=f\"batch-predict-{MODEL_ID}\"\n",
        ")\n",
        "\n",
        "text_batch_predict_job = models[\"model\"].batch_predict(\n",
        "    job_display_name=batch_predict_job_name,\n",
        "    gcs_source=os.path.join(\n",
        "        BUCKET_URI, batch_predict_prefix, f\"input/{instances_filename}\"\n",
        "    ),\n",
        "    gcs_destination_prefix=os.path.join(\n",
        "        BUCKET_URI, batch_predict_prefix, \"output\"\n",
        "    ),\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        ")\n",
        "\n",
        "text_batch_predict_job.wait()\n",
        "\n",
        "print(text_batch_predict_job.display_name)\n",
        "print(text_batch_predict_job.resource_name)\n",
        "print(text_batch_predict_job.state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mV0gKSKjG2DZ"
      },
      "outputs": [],
      "source": [
        "# @title #### Get inference results\n",
        "\n",
        "# @markdown This section shows an example of [retrieving batch inference results](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions#retrieve_batch_prediction_results) from the JSON Lines file(s) in the output Cloud Storage location.\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details.\n",
        "\n",
        "def download_gcs_files_as_json(gcs_files_prefix):\n",
        "    \"\"\"Download specified files from Cloud Storage and convert content to JSON.\"\"\"\n",
        "    lines = []\n",
        "    client = storage.Client()\n",
        "    bucket = storage.bucket.Bucket.from_string(BUCKET_NAME, client)\n",
        "    blobs = bucket.list_blobs(prefix=gcs_files_prefix)\n",
        "    for blob in blobs:\n",
        "        with blob.open(\"r\") as f:\n",
        "            for line in f:\n",
        "                lines.append(json.loads(line))\n",
        "    return lines\n",
        "\n",
        "\n",
        "# Get results from the first batch inference job (with multimodal inputs)\n",
        "# You can replace this variable to get results from another batch inference job\n",
        "batch_predict_job = multimodal_batch_predict_job\n",
        "batch_predict_output_dir = batch_predict_job.output_info.gcs_output_directory\n",
        "batch_predict_output_files_prefix = os.path.join(\n",
        "    batch_predict_output_dir.replace(f\"{BUCKET_NAME}/\", \"\"),\n",
        "    \"prediction.results\"\n",
        ")\n",
        "batch_predict_results = download_gcs_files_as_json(\n",
        "    gcs_files_prefix=batch_predict_output_files_prefix\n",
        ")\n",
        "\n",
        "# Display first batch inference result\n",
        "line = batch_predict_results[0]\n",
        "prediction = line[\"prediction\"][\"predictions\"][\"choices\"][0][\"message\"][\"content\"]\n",
        "display(Markdown(prediction))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yfFlkncxzDF"
      },
      "source": [
        "\n",
        "## Next steps\n",
        "\n",
        "Explore the other [notebooks](https://github.com/google-health/medgemma/blob/main/notebooks) to learn what else you can do with the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paQNyrzT_mX_"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "edUIpvZZ_mYA"
      },
      "outputs": [],
      "source": [
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continuous charges that may incur.\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "for endpoint in endpoints.values():\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "for model in models.values():\n",
        "    model.delete()\n",
        "\n",
        "delete_bucket = False  # @param {type:\"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "quick_start_with_model_garden.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
