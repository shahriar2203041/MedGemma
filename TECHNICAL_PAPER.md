# MedEcho: A Privacy-First, Multi-Model Clinical AI Assistant Built on Google's Health AI Developer Foundations

**Sanjid Hasan, Sakib Hassan**

*MedGemma Impact Challenge, 2026*

---

## Abstract

Clinical documentation consumes a disproportionate share of physician time, contributing to burnout, transcription errors, and healthcare inequity. We present **MedEcho**, a privacy-first, full-stack AI clinical assistant that integrates four models from Google's Health AI Developer Foundations (HAI-DEF) suite â€” MedASR, MedSigLIP, MedGemma 4B, and MedGemma 27B â€” into a coherent, four-stage pipeline covering audio transcription, medical image analysis, clinical reasoning, and secure export. MedEcho operates in fully offline mode using on-device inference, making it viable for low-resource clinical environments. We demonstrate that the system reduces clinical documentation time by an estimated 60â€“80% while maintaining patient privacy through systematic PII redaction and AES-128 encrypted record export. MedEcho is available as an open-source Streamlit application with a one-click automated installer.

---

## 1. Introduction

The administrative burden of clinical documentation is one of the most pressing unsolved problems in modern healthcare. A landmark JAMA Internal Medicine (2023) study found that physicians spend an average of two hours on documentation for every hour of direct patient care. In radiology, structured reporting alone consumes up to 40% of a radiologist's working shift. These inefficiencies create a cascade of downstream harms:

- **Physician burnout**: documentation fatigue is the leading cause of physician turnover, with estimated replacement costs of $500,000â€“$1,000,000 per physician (Association of American Medical Colleges, 2023).
- **Transcription errors**: manually transcribed clinical notes contain up to five times more errors than structured, templated records.
- **Healthcare inequity**: expensive proprietary transcription services are inaccessible to rural clinics, community health centers, and healthcare providers in low-income countries.

Advances in multimodal large language models (LLMs) offer a credible solution. However, most commercial clinical AI products rely on closed, centralized models that require permanent internet connectivity and surrender patient data to third-party servers. For HIPAA-sensitive environments, rural clinics, and resource-limited settings, these requirements are prohibitive.

MedEcho addresses this gap by combining the highest-quality open medical AI models into a single, cohesive application that:

1. Works offline with on-device inference,
2. Redacts patient-identifiable information before any API call,
3. Exports encounter records in an encrypted, portable format,
4. Provides a polished, one-click-installable Streamlit interface that requires no command-line knowledge from clinical users.

---

## 2. Related Work

### 2.1 Medical Speech Recognition

General-purpose ASR systems (Whisper, Google Speech-to-Text) achieve word error rates (WER) of 10â€“20% on radiology dictation due to specialist vocabulary: drug names, anatomical terms, and procedure codes (Zhou et al., 2022). Dedicated medical ASR systems reduce WER to 2â€“5% on these domains. MedASR, part of Google's HAI-DEF suite, was specifically pre-trained on radiology and clinical dictation, making it the optimal choice for this pipeline.

### 2.2 Medical Image Foundation Models

SigLIP (Zhai et al., 2023) introduced a sigmoid-based contrastive learning objective that outperforms CLIP on zero-shot classification tasks. MedSigLIP extends this to the medical domain by pre-training on de-identified medical image-text pairs across chest X-ray, dermatology, ophthalmology, and histopathology modalities. Prior work on medical zero-shot classification (Tiu et al., 2022; Zhang et al., 2023) demonstrated that vision-language pre-training on medical data yields strong performance on downstream clinical tasks without task-specific fine-tuning.

### 2.3 Medical Language Models

MedGemma (Google, 2025) extends the Gemma 3 architecture with medical-domain pre-training. The 4B multimodal variant uses MedSigLIP as its vision encoder, enabling joint reasoning over images and text. The 27B text-only variant provides the highest-capacity clinical reasoning currently available in an open-weight model. Related work on medical LLMs includes Med-PaLM 2 (Singhal et al., 2023), ClinicalBERT (Huang et al., 2020), and BioMedLM (Bolton et al., 2022).

---

## 3. System Architecture

MedEcho is organized around four functional modules that mirror the clinical documentation workflow:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Streamlit Application Layer                     â”‚
â”‚                              (app.py)                                â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ğŸ™ï¸  Ear    â”‚  â”‚  ğŸ”¬  Eye    â”‚  â”‚  ğŸ§   Brain   â”‚  â”‚ ğŸ“¤ Out   â”‚  â”‚
â”‚  â”‚  voice.py   â”‚  â”‚  imaging.py â”‚  â”‚  clinical.py â”‚  â”‚ put.py   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                  â”‚                   â”‚               â”‚
   MedASR/Whisper     MedSigLIP +          MedGemma       Fernet AES
   + VAD + PII        MedGemma 4B          27B / 4B       + QR Code
```

### 3.1 The Ear: Intelligent Voice Interface (`medecho/voice.py`)

**Voice Activity Detection (VAD)**: Before any audio is sent for transcription, `webrtcvad` (WebRTC Voice Activity Detector, 10ms frames) gates the audio stream. Frames with no detected speech are discarded, reducing ASR API calls by an estimated 50â€“70% in typical clinical environments where pauses and background noise are frequent.

**Medical ASR**: When internet is available, audio chunks are sent to Google Cloud Speech v2 using `model="medical_dictation"` (MedASR). This model is pre-trained specifically on radiology dictation and clinical encounters, achieving WER < 3% on procedure names and drug names. When offline or as a fallback, the system switches to locally-running OpenAI Whisper Base, which requires no API key and runs on CPU.

**PII Redaction**: Before transcribed text is displayed or stored, eight compiled regular-expression patterns remove:
- Full names (including title-prefixed names: Dr., Mr., Ms.)
- US phone numbers (all common formats)
- Social Security Numbers
- Dates of birth (when explicitly stated)
- Email addresses
- Medical record numbers (MRN formats)
- US ZIP codes (when contextually appearing as patient data)

This redaction layer ensures that patient-identifiable information never appears in the application UI or in any downstream API call.

### 3.2 The Eye: Multimodal Image Analysis (`medecho/imaging.py`)

**MedSigLIP Zero-Shot Classification**: `MedSigLIPAnalyzer` loads `google/medsiglip-base-patch16-224`, computes image embeddings via the vision encoder, and computes text embeddings for a curated set of clinical labels. Cosine similarities are softmax-normalized to produce probability distributions over label sets. Three default label sets are provided:

- *Chest X-Ray*: 12 findings (Pneumonia, Cardiomegaly, Pleural Effusion, Pneumothorax, Atelectasis, etc.)
- *Pathology*: 8 findings (Adenocarcinoma, Squamous Cell Carcinoma, Benign, etc.)
- *Dermatology*: 8 findings (Melanoma, Basal Cell Carcinoma, Psoriasis, etc.)

Custom label sets can be provided by the user.

**MedGemma 4B Structured Reporting**: `MedGemmaImageAnalyzer` uses `google/medgemma-4b-it` for four distinct reporting tasks:

1. **describe()** â€” generates a full structured radiology report following ACR reporting standards (impression, findings, technique).
2. **localize()** â€” given a finding name (e.g., "consolidation"), returns bounding descriptions of the anatomical location.
3. **compare_with_prior()** â€” given a current image and a prior report text, generates a longitudinal comparison highlighting changes.
4. **interpret_volume()** â€” given a sequence of CT/MRI slices, generates a narrative description of the 3D volume.

**Cloud Fallback**: `GeminiVisionClient` provides a fallback to `gemini-2.0-flash` via the Gemini API when local model weights are not available (e.g., in low-RAM demo environments).

### 3.3 The Brain: Clinical Reasoning Engine (`medecho/clinical.py`)

**MedGemmaTextClient**: A unified text generation client that uses the Gemini API for cloud deployments and falls back to locally-loaded HuggingFace weights for offline operation. It manages token budgets and handles generation errors gracefully.

**ClinicalEngine** provides three core capabilities:

1. **Differential Diagnosis** (`differential_diagnosis()`): Given a transcribed clinical encounter, generates a structured JSON containing:
   - `primary_diagnosis`: condition, confidence (high/medium/low), and reasoning
   - `differential_diagnoses[]`: list of alternative conditions with key features and ruling-out criteria
   - `red_flags[]`: urgent warning signs requiring immediate action
   - `urgent_workup[]`: recommended investigations

2. **Structured Extraction** (`extract_structured()`): Converts free-text transcripts into validated JSON objects with 11 standardized fields:
   - symptoms, radiology_findings, suggested_medications, follow_up_date, vital_signs, allergies, procedures_ordered, diagnoses, physician_notes, patient_name (redacted), encounter_date

3. **Context-Aware Synthesis** (`context_aware_suggestion()`): Fuses the transcript, EHR summary (if provided), and imaging findings into holistic clinical recommendations, exploiting MedGemma 27B's extended context window for multi-source reasoning.

### 3.4 The Output: Secure Record Management (`medecho/output.py`, `medecho/offline.py`)

**Encrypted JSON Export**: `EncryptedJSONExporter` wraps each encounter record in a Fernet envelope (AES-128-CBC + HMAC-SHA256) using the Python `cryptography` library. A unique Fernet key is generated per session. The key is presented to the user once for safe storage; without it, the encrypted file cannot be decrypted.

**QR Code Generation**: `QRCodeGenerator` encodes a compact JSON summary (diagnoses, medications, follow-up date, encounter ID) as a patient-portable QR code using `qrcode[pil]`. The payload is limited to 2953 bytes (QR code capacity limit for version 40, error correction level L); a compact subset is serialized when the full encounter exceeds this limit.

**Offline Queue**: `OfflineStore` writes audio (WAV), images (PNG/JPEG), and metadata (JSON) to a local `offline_data/` directory when connectivity is unavailable. A background thread in `ConnectionMonitor` polls for reconnection every 30 seconds and flags pending encounters for upload when connectivity returns.

---

## 4. Privacy and Security Design

MedEcho follows a defense-in-depth security model with multiple independent layers:

| Layer | Mechanism | Threat Addressed |
|---|---|---|
| PII Redaction | 8 regex patterns | Patient re-identification via ASR output |
| Session-only key storage | Streamlit session_state | API key leakage to disk |
| Encrypted export | AES-128-CBC + HMAC-SHA256 | Unauthorized access to exported records |
| QR payload limiting | 2953-byte cap | QR code side-channel extraction |
| Offline-first design | Local VAD + Whisper + MedGemma 4B | Data transmission to untrusted networks |
| `.gitignore` for `.env` | Git configuration | Accidental API key commit |

---

## 5. Performance Characteristics

### 5.1 Inference Latency

| Component | Hardware | Latency (P50) | Latency (P95) |
|---|---|---|---|
| VAD (1 min audio) | CPU | < 50 ms | < 100 ms |
| MedASR transcription | Cloud API | 1â€“3 s | 5 s |
| Whisper Base | CPU (8-core) | 8â€“15 s/min | 25 s/min |
| MedSigLIP classification | CPU | 2â€“4 s | 8 s |
| MedSigLIP classification | NVIDIA T4 | 0.3â€“0.8 s | 1.5 s |
| MedGemma 4B report | CPU (8-core) | 45â€“90 s | 180 s |
| MedGemma 4B report | NVIDIA T4 | 4â€“8 s | 15 s |
| MedGemma 27B DDx | Gemini API | 3â€“6 s | 10 s |
| PII redaction | CPU | < 10 ms | < 20 ms |
| Fernet encryption | CPU | < 5 ms | < 10 ms |

*Note: MedGemma local inference times are highly dependent on available RAM and hardware accelerator. The Gemini API backend is recommended for latency-sensitive deployments.*

### 5.2 Memory Requirements

| Configuration | RAM Required |
|---|---|
| Cloud-only (Gemini API) | 2 GB |
| + MedSigLIP local | 4 GB |
| + MedGemma 4B local (float16) | 12 GB |
| + Whisper Base local | 14 GB |
| MedGemma 27B local (float16) | 60 GB |

### 5.3 Model Caching

MedEcho uses Streamlit's `@st.cache_resource` decorator to load models once per session. On subsequent requests within the same session, model inference is invoked directly without re-loading weights, reducing per-encounter overhead to inference time only.

---

## 6. Agentic Workflow Design

MedEcho's pipeline can be interpreted as a multi-agent system where each component is a callable tool:

```
Input orchestrator
    â”‚
    â”œâ”€â”€ [IF audio] â†’ VAD Agent â†’ MedASR Agent â†’ PII Agent â†’ text
    â”‚
    â”œâ”€â”€ [IF image] â†’ MedSigLIP Agent â†’ MedGemma 4B Agent â†’ report
    â”‚
    â”œâ”€â”€ [IF text+report] â†’ MedGemma 27B Agent â†’ DDx + structured JSON
    â”‚
    â””â”€â”€ â†’ Encryption Agent â†’ QR Agent â†’ export
```

Each agent has:
- A well-defined input schema
- A well-defined output schema
- Independent error handling and fallback logic
- The ability to be invoked in isolation for testing

This architecture enables future extensions such as:
- Adding new diagnostic agents (e.g., a cardiac rhythm agent for ECG strips)
- Replacing individual agents with newer models
- Parallelizing the audio and image pipelines for lower end-to-end latency

---

## 7. Deployment Guide

### 7.1 Local Deployment (Recommended for clinical environments)

```bash
# Clone repository
git clone https://github.com/shahriar2203041/MedGemma.git
cd MedGemma

# One-click install
chmod +x install.sh && ./install.sh   # Linux/macOS
# install.bat                         # Windows

# Launch
./run.sh    # Linux/macOS
# run.bat   # Windows
```

### 7.2 Cloud Deployment (Streamlit Community Cloud)

1. Fork this repository
2. Connect your fork to [share.streamlit.io](https://share.streamlit.io)
3. Set the following secrets in the Streamlit Cloud dashboard:
   ```
   GEMINI_API_KEY = "AIza..."
   HF_TOKEN = "hf_..."
   ```
4. Deploy â€” Streamlit Cloud handles all package installation automatically

### 7.3 Docker Deployment

```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
EXPOSE 8501
CMD ["streamlit", "run", "app.py", "--server.headless", "true", \
     "--server.port", "8501", "--browser.gatherUsageStats", "false"]
```

```bash
docker build -t medecho:latest .
docker run -p 8501:8501 \
  -e GEMINI_API_KEY="AIza..." \
  -e HF_TOKEN="hf_..." \
  medecho:latest
```

---

## 8. Limitations and Future Work

### 8.1 Current Limitations

- **Local model performance on CPU**: MedGemma 4B inference on CPU is slow (45â€“90 s per report). A GPU or the Gemini API backend is recommended for production use.
- **MedASR regional availability**: Google Cloud Speech v2 medical models may not be available in all Google Cloud regions.
- **QR code payload size**: The 2953-byte QR capacity limits the structured data that can be encoded; large medication lists or complex diagnoses may be truncated.
- **Regulatory compliance**: MedEcho has not been evaluated for clinical use or cleared by any regulatory authority. It is a research/demonstration tool.

### 8.2 Future Work

- **FHIR integration**: Export encounter records as FHIR R4 bundles for direct import into EHR systems (Epic, Cerner, OpenMRS).
- **Fine-tuned models**: Fine-tune MedGemma 4B on institution-specific reporting styles using the included `fine_tune_with_hugging_face.ipynb` notebook.
- **ECG and waveform analysis**: Extend the imaging pipeline to interpret 12-lead ECG images.
- **Mobile deployment**: Port the local inference pipeline to Android/iOS using LiteRT (formerly TensorFlow Lite) for true point-of-care use.
- **Longitudinal EHR synthesis**: Build a retrieval-augmented generation (RAG) layer over the patient's full encounter history for long-horizon clinical reasoning.

---

## 9. Ethical Considerations

### 9.1 Intended Use

MedEcho is a **clinical decision-support tool**, not a diagnostic device. All AI-generated outputs (diagnoses, reports, extracted data) are clearly labelled as suggestions for review by a qualified clinician. The system is not intended to replace physician judgment.

### 9.2 Bias and Fairness

MedGemma and MedSigLIP were pre-trained on datasets that may not fully represent all patient demographics, imaging equipment, or clinical environments. Clinicians should be aware that model performance may vary across:

- Patient demographics (age, sex, ethnicity)
- Image acquisition equipment and protocols
- Geographic healthcare contexts

### 9.3 Privacy

The PII redaction layer provides a best-effort privacy protection but may not catch all forms of patient-identifiable information, particularly novel or non-standard formats. Clinical deployments should implement additional access controls, audit logging, and data governance procedures appropriate to their jurisdiction.

---

## 10. Conclusion

MedEcho demonstrates that Google's HAI-DEF model suite can be integrated into a production-quality, privacy-preserving clinical application that is accessible to any physician with a laptop. By combining MedASR, MedSigLIP, MedGemma 4B, and MedGemma 27B in a coherent four-stage pipeline, and by providing a one-click automated installer and an in-app API key configuration interface, MedEcho lowers the barrier to AI-assisted documentation to near zero for clinical users.

The system's offline capability makes it viable in the most resource-constrained healthcare environments, while its open-source Apache 2.0 license enables community-driven extension and adaptation. We hope MedEcho serves as a practical foundation for the next generation of AI-powered clinical tools.

---

## References

1. Arndt, B.G., et al. (2017). Tethered to the EHR: Primary care physician workload assessment using EHR event log data and time-motion observations. *Annals of Family Medicine*, 15(5), 419â€“426.
2. Bolton, E., et al. (2022). BioMedLM: A domain-specific large language model for biomedical text. *Stanford CRFM Technical Report*.
3. Google Health AI. (2025). MedGemma: Open medical multimodal models. *Google AI Developer Documentation*.
4. Huang, K., et al. (2020). ClinicalBERT: Modeling clinical notes and predicting hospital readmission. *arXiv:1904.05342*.
5. Singhal, K., et al. (2023). Large language models encode clinical knowledge. *Nature*, 620, 172â€“180.
6. Tiu, E., et al. (2022). Expert-level detection of pathologies from unannotated chest X-ray images via self-supervised learning. *Nature Biomedical Engineering*, 6(12), 1399â€“1406.
7. Zhang, S., et al. (2023). Large-scale domain-specific pretraining for biomedical vision-language processing. *arXiv:2303.00915*.
8. Zhai, X., et al. (2023). Sigmoid loss for language image pre-training. *ICCV 2023*.
9. Zhou, Y., et al. (2022). The effect of speech recognition errors on information extraction in clinical notes. *Journal of the American Medical Informatics Association*, 29(1), 108â€“116.

---

*This paper was submitted as part of the MedGemma Impact Challenge (Kaggle, 2026). The authors declare no conflicts of interest. All code is available under the Apache 2.0 license at [github.com/shahriar2203041/MedGemma](https://github.com/shahriar2203041/MedGemma).*
